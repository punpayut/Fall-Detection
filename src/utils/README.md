# Fall Detection Video Dataset Processing

This directory contains a set of Python scripts to process video data for a fall detection system. The pipeline involves extracting keypoints from videos, processing these keypoints into sequences, splitting the dataset, and counting the generated sequences.

## Scripts Overview

### 1. `extract_keypoint.py`
   - **Purpose**: Extracts human body keypoints from raw video files.
   - **Functionality**:
     - Uses the MediaPipe Pose library for pose estimation.
     - Iterates through video files (`.mp4`, `.avi`, `.mov`) located in `Fall/Raw_Video` and `No_Fall/Raw_Video` subdirectories.
     - For each video, it extracts 17 specific keypoints (e.g., nose, shoulders, hips, knees, ankles) for each frame.
     - Saves the X, Y coordinates and visibility/confidence of these keypoints into a CSV file (e.g., `video_name_keypoints.csv`) in a corresponding `Keypoints_CSV` folder (`Fall/Keypoints_CSV` or `No_Fall/Keypoints_CSV`).
     - Avoids reprocessing if a CSV file for a video already exists.
   - **Input**: Raw video files in `./Fall/Raw_Video/` and `./No_Fall/Raw_Video/`.
   - **Output**: CSV files containing keypoint data in `./Fall/Keypoints_CSV/` and `./No_Fall/Keypoints_CSV/`.

### 2. `process_csv_data.py`
   - **Purpose**: Processes the extracted keypoint CSV files into fixed-length sequences suitable for model training.
   - **Functionality**:
     - Reads each CSV file generated by `extract_keypoint.py`.
     - Pivots the data so that keypoint coordinates (X, Y, Confidence) become features for each frame.
     - Handles missing frames by filling them with zeros.
     - Creates overlapping sequences of a fixed length (default: 30 frames) from the frame features.
     - Uses different step sizes for "Fall" (default: 3) and "No_Fall" (default: 6) categories to generate sequences, allowing for more "Fall" instances.
     - Applies zero-padding if a video is shorter than the sequence length or if the last segment is shorter.
     - Saves each generated sequence as a separate `.npy` file in a temporary directory: `./Model_Ready_Dataset/_temp_npy/`.
     - Creates a JSON file (`./Model_Ready_Dataset/all_sequences_info.json`) that lists the paths to all generated `.npy` sequence files and their labels (1 for Fall, 0 for No_Fall).
   - **Input**: CSV files from `./Fall/Keypoints_CSV/` and `./No_Fall/Keypoints_CSV/`.
   - **Output**:
     - `.npy` sequence files in `./Model_Ready_Dataset/_temp_npy/`.
     - `all_sequences_info.json` in `./Model_Ready_Dataset/`.

### 3. `split_dataset.py`
   - **Purpose**: Splits the generated sequences into training, validation, and test sets and organizes them into a structured directory.
   - **Functionality**:
     - Loads the list of `.npy` sequence file paths and their labels from `all_sequences_info.json`.
     - Splits the sequences into training, validation, and test sets based on specified ratios (default: 70% train, 15% validation, 15% test).
     - Uses stratified splitting to try and maintain class proportions within each set.
     - Copies the `.npy` sequence files from the temporary directory into a structured output directory: `./Model_Ready_Dataset/{train|val|test}/{fall|no_fall}/`.
     - Attempts to remove the temporary `_temp_npy` directory after copying.
     - Creates a `summary.csv` file in `./Model_Ready_Dataset/` that lists all sequences, their final paths, labels, and the split they belong to.
   - **Input**:
     - `./Model_Ready_Dataset/all_sequences_info.json`.
     - `.npy` files in `./Model_Ready_Dataset/_temp_npy/`.
   - **Output**:
     - Organized dataset in `./Model_Ready_Dataset/train/`, `./Model_Ready_Dataset/val/`, `./Model_Ready_Dataset/test/`.
     - `./Model_Ready_Dataset/summary.csv`.

### 4. `count_processed_sequences.py`
   - **Purpose**: Counts the number of processed sequences for each class (Fall/No_Fall).
   - **Functionality**:
     - Reads the `all_sequences_info.json` file.
     - Counts the number of sequences for "Fall" and "No_Fall" labels.
     - Prints the counts and the total number of sequences to the console.
     - Useful for verifying the output of `process_csv_data.py`.
   - **Input**: `./Model_Ready_Dataset/all_sequences_info.json`.
   - **Output**: Prints counts to the console.

## Expected Directory Structure (before running scripts)

```
.
├── Fall/
│   └── Raw_Video/
│       ├── fall_video_1.mp4
│       └── ...
├── No_Fall/
│   └── Raw_Video/
│       ├── no_fall_video_1.mp4
│       └── ...
├── extract_keypoint.py
├── process_csv_data.py
├── split_dataset.py
└── count_processed_sequences.py
```

## Expected Directory Structure (after running all scripts)

```
.
├── Fall/
│   ├── Raw_Video/
│   │   ├── fall_video_1.mp4
│   │   └── ...
│   └── Keypoints_CSV/
│       ├── fall_video_1_keypoints.csv
│       └── ...
├── No_Fall/
│   ├── Raw_Video/
│   │   ├── no_fall_video_1.mp4
│   │   └── ...
│   └── Keypoints_CSV/
│       ├── no_fall_video_1_keypoints.csv
│       └── ...
├── Model_Ready_Dataset/
│   ├── train/
│   │   ├── fall/
│   │   │   └── fall_video_X_seq_Y.npy
│   │   └── no_fall/
│   │       └── no_fall_video_X_seq_Y.npy
│   ├── val/
│   │   ├── fall/
│   │   └── no_fall/
│   ├── test/
│   │   ├── fall/
│   │   └── no_fall/
│   ├── all_sequences_info.json
│   ├── summary.csv
│   ├── process_csv_data.log  (Log file from process_csv_data.py)
│   └── split_dataset.log     (Log file from split_dataset.py)
├── extract_keypoint.py
├── process_csv_data.py
├── split_dataset.py
├── count_processed_sequences.py
└── README.md
```

## How to Run

1.  **Ensure Dependencies**: Make sure you have Python installed along with the necessary libraries (OpenCV, MediaPipe, NumPy, Pandas, scikit-learn). You can typically install them using pip:
    ```bash
    pip install opencv-python mediapipe numpy pandas scikit-learn
    ```
2.  **Prepare Data**: Place your raw video files into the `Fall/Raw_Video/` and `No_Fall/Raw_Video/` directories.
3.  **Run Scripts in Order**:
    ```bash
    python extract_keypoint.py
    python process_csv_data.py
    python split_dataset.py
    python count_processed_sequences.py
    ```
    It's recommended to run them sequentially as each script depends on the output of the previous one.

## Configuration

The scripts contain some configurable parameters at the top of each file:
-   `extract_keypoint.py`: `KEYPOINT_INDICES` for selecting specific keypoints.
-   `process_csv_data.py`: `INPUT_ROOT_DIR`, `OUTPUT_ROOT_DIR`, `SEQUENCE_LENGTH`, `STEP_SIZE_FALL`, `STEP_SIZE_NO_FALL`.
-   `split_dataset.py`: `OUTPUT_ROOT_DIR`, `SPLIT_RATIOS`.
-   `count_processed_sequences.py`: `MODEL_READY_DATASET_DIR`.

Adjust these as needed for your specific dataset or requirements.
